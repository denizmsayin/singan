{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "twzxTwPGuL49"
   },
   "source": [
    "# Paper Information\n",
    "Paper: SinGAN: Learning a Generative Model from a Single Natural Image, https://arxiv.org/abs/1905.01164\n",
    "\n",
    "Authors: Tamar Rott Shaham, Tali Dekel, Tomer Michaeli\n",
    "\n",
    "Code Authors: Ataberk Dönmez, Deniz Sayın "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cgcy-baFuwS6"
   },
   "source": [
    "# Initialization & Hyperparameters\n",
    "\n",
    "The module imports and device selection sections must always be run before being able to run any of the other sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BEJgmtGnvuaV"
   },
   "source": [
    "## Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 941,
     "status": "ok",
     "timestamp": 1590251405361,
     "user": {
      "displayName": "Deniz MERCADIER SAYIN",
      "photoUrl": "",
      "userId": "12243424802177233752"
     },
     "user_tz": -180
    },
    "id": "RPuukm-880dd",
    "outputId": "66a097bc-e8d5-400b-cdc6-6972f928e0a2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess as sp\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# our own helper modules\n",
    "from models import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vXu94rFCHFSJ"
   },
   "source": [
    "## Device Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WEPyQPmpHFSL"
   },
   "outputs": [],
   "source": [
    "# arguments cell\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4vjXWJjHFSX"
   },
   "source": [
    "## Training Hyperparameters\n",
    "\n",
    "Please note that running this cell and setting the parameters is only necessary for training, generating the results does not require any of these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQbM5wSAgu8z"
   },
   "outputs": [],
   "source": [
    "# arguments cell\n",
    "# image to train on\n",
    "IMG_PATH = 'images/birds.png'\n",
    "SAVE_PATH = 'models/birds.pt'\n",
    "\n",
    "# training hyperparameters, \n",
    "# as given in the paper\n",
    "GEN_LEARNING_RATE = 0.0005\n",
    "CRIT_LEARNING_RATE = 0.0005\n",
    "BETA_1 = 0.5  # beta parameters for the ADAM optimizer\n",
    "BETA_2 = 0.999\n",
    "NUM_ITERS = 2000  # number of iterations at each scale\n",
    "LR_DROP_STEP = 1600  # step at which to decay learning rate\n",
    "LR_DROP_MULT = 0.1  # lr decay multiplier\n",
    "GEN_STEP_PER_ITER = 3  # optimization step per iteration for the generator\n",
    "CRIT_STEP_PER_ITER = 3  # ... and the critic\n",
    "REC_ALPHA = 10.0  # reconstruction loss weight\n",
    "GP_WEIGHT = 0.1  # gradient penalty weight\n",
    "\n",
    "# architecture details\n",
    "NUM_SCALES = 9  # number of training scales, the most important parameter!\n",
    "KERNEL_SIZE = 3  # kernel size of the convolutions, no need to change\n",
    "NUM_BLOCKS = 5  # number of blocks in each network, no need to change\n",
    "INITIAL_KERNEL_COUNT = 32  # the initial amount of kernels in each conv layer\n",
    "INCREASE_KERNEL_COUNT_EVERY = 4  # ... and how often to double their amount\n",
    "NOISE_BASE_STD = 0.5  # base noise stdev multiplier at upper scales (scaled with rmse), different values in the range [0.1, 1.0] seem to work\n",
    "FIRST_SCALE_NOISE_STD = 1.0  # noise stdev at the first scale, standard normal, different because there is no input image\n",
    "MAX_INPUT_SIZE = 250  # if the input image is larger than this, resize its long edge\n",
    "UPSAMPLING_FACTOR = 4/3  # the factor by which images are upsampled at each scale\n",
    "UPSAMPLING_MODE = 'bilinear'  # mode used when upscaling during training\n",
    "DOWNSAMPLING_MODE = 'bicubic'  # mode used when downscaling the original input\n",
    "\n",
    "# extra settings\n",
    "PRINT_EVERY = 500  # show progress every X iterations\n",
    "SEED = 796  # random seed value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fxWUuRjzxmBT"
   },
   "source": [
    "# Training & Saving a Model\n",
    "\n",
    "Takes between half an hour and a few hours depending on the GPU you have access to. Training a full model using only a CPU is not viable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kfPett90HFSh"
   },
   "outputs": [],
   "source": [
    "# training cell\n",
    "# closures for easy use depending on settings, so that\n",
    "# we can skip providing every single argument at each iteration\n",
    "def make_generator(kernel_count, noise_std):\n",
    "  sgnet = SGNet(NUM_BLOCKS, kernel_count, KERNEL_SIZE, final_activation=nn.Tanh(), output_channels=3).to(DEVICE)\n",
    "  return SGGen(sgnet, noise_std)\n",
    "  \n",
    "def make_critic(kernel_count):\n",
    "  return SGNet(NUM_BLOCKS, kernel_count, KERNEL_SIZE, final_activation=None, output_channels=1).to(DEVICE)\n",
    "\n",
    "def make_optimizer_and_scheduler(net, net_learning_rate):\n",
    "  optimizer = torch.optim.Adam(net.parameters(), net_learning_rate, (BETA_1, BETA_2))\n",
    "  sched = torch.optim.lr_scheduler.StepLR(optimizer, LR_DROP_STEP, LR_DROP_MULT)\n",
    "  return optimizer, sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5UpI7xyUYJoo"
   },
   "outputs": [],
   "source": [
    "# training cell\n",
    "# load the image along with its downsampled versions and their exact sizes\n",
    "downsampling_factor = 1.0 / UPSAMPLING_FACTOR\n",
    "scaled_origs, exact_sizes = load_with_reverse_pyramid(IMG_PATH, MAX_INPUT_SIZE, downsampling_factor, NUM_SCALES, \n",
    "                                                      mode=DOWNSAMPLING_MODE, device=DEVICE, verbose=True)\n",
    "  \n",
    "original_image = scaled_origs[-1]\n",
    "print('Input image:')\n",
    "plt.imshow(normed_tensor_to_np_image(original_image))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtiPtGz8x5lN"
   },
   "outputs": [],
   "source": [
    "# training cell\n",
    "# seed stuff\n",
    "seed_rngs(SEED)\n",
    "\n",
    "# create the scaled images\n",
    "coarsest_exact_size = exact_sizes[0]\n",
    "\n",
    "# initialize the constant noise used in reconstruction\n",
    "z_rec_coarsest = FIRST_SCALE_NOISE_STD * torch.randn_like(scaled_origs[0], device=DEVICE)\n",
    "z_rec = [z_rec_coarsest] # a zero tensor is appended after each scale\n",
    "\n",
    "# constant zero input for the coarsest scale during training\n",
    "coarsest_zero_input = torch.zeros_like(z_rec_coarsest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6cDWoeR6fQtS"
   },
   "outputs": [],
   "source": [
    "# training cell\n",
    "# loop values\n",
    "training_start = time()\n",
    "kernel_count = INITIAL_KERNEL_COUNT\n",
    "generators, critics = [], []\n",
    "gen_losses, crit_losses = [], []\n",
    "for scale_index in range(NUM_SCALES):\n",
    "  print('****************************\\nScale {}'.format(scale_index))\n",
    "\n",
    "  # get the original image at the current scale\n",
    "  scale_orig_img = scaled_origs[scale_index]\n",
    "\n",
    "  # things to be done after the first scale\n",
    "  if scale_index > 0:\n",
    "    # use RMSE to determine the standard deviation of the input noise\n",
    "    with torch.no_grad():\n",
    "      reconstruction = generator(z_rec)  # specific reconstruction noise\n",
    "    scaled_reconstruction, _ = exact_interpolate(reconstruction, UPSAMPLING_FACTOR, exact_sizes[scale_index-1], UPSAMPLING_MODE)\n",
    "    rmse = torch.sqrt(F.mse_loss(scaled_reconstruction, scale_orig_img))\n",
    "    print('RMSE: {:.2f}'.format(rmse))\n",
    "    # if the scale matches, increase kernel count\n",
    "    if scale_index % INCREASE_KERNEL_COUNT_EVERY == 0:\n",
    "      kernel_count *= 2\n",
    "    # add a zero tensor to the reconstruction noise list\n",
    "    # since it is defined as [z*, 0, 0, 0...] for some z*\n",
    "    z_rec.append(torch.zeros_like(scale_orig_img))\n",
    "      \n",
    "  # create the noise sampler based on the RMSE\n",
    "  # the first scale's stdev is different due to the zero input,\n",
    "  # the noise has to be stronger than in the upper layers, although\n",
    "  # this is alleviated by the rmse multiplier even if \n",
    "  # FIRST_SCALE_NOISE_STD == NOISE_BASE_STD\n",
    "  scale_noise_std = FIRST_SCALE_NOISE_STD if scale_index == 0 else rmse * NOISE_BASE_STD\n",
    "\n",
    "  ## initialize the generator\n",
    "  # create a generator for this specific scale and initialize it\n",
    "  scale_generator = make_generator(kernel_count, scale_noise_std)\n",
    "  # copy weights from previous if possible, and add to the list\n",
    "  initialize_net(scale_generator, generators)\n",
    "  \n",
    "  # create a single generator view from the stack of generators\n",
    "  generic_generator = MultiScaleSGGenView(generators, UPSAMPLING_FACTOR, UPSAMPLING_MODE)\n",
    "  # fix the input parameters for easier forward calls\n",
    "  generator = FixedInputSGGenView(generic_generator, coarsest_zero_input, coarsest_exact_size)\n",
    "  \n",
    "  ## initialize the critic (discriminator)\n",
    "  critic = make_critic(kernel_count)\n",
    "  initialize_net(critic, critics)\n",
    "\n",
    "  # create the optimizers and schedulers\n",
    "  gen_optimizer, gen_sched = make_optimizer_and_scheduler(generator, GEN_LEARNING_RATE)\n",
    "  crit_optimizer, crit_sched = make_optimizer_and_scheduler(critic, CRIT_LEARNING_RATE)\n",
    "\n",
    "  # print norms to ensure correct operation\n",
    "  gen_norms = ['G{}: {:.3f}'.format(i, sum_param_norms(g)) for i, g in enumerate(generators)]\n",
    "  crit_norms = ['C{}: {:.3f}'.format(i, sum_param_norms(c)) for i, c in enumerate(critics)]\n",
    "  print('Generator norms: ' + ', '.join(gen_norms))\n",
    "  print('Critic norms: ' + ', '.join(crit_norms))\n",
    "  \n",
    "  # perform training\n",
    "  for step in range(NUM_ITERS):\n",
    "\n",
    "    for _ in range(CRIT_STEP_PER_ITER):\n",
    "      crit_optimizer.zero_grad()\n",
    "      \n",
    "      # the model handles noise sampling on its own\n",
    "      fake_img = generator()\n",
    "      \n",
    "      # gradient & adversarial loss\n",
    "      grad_loss = gradient_penalty(critic, fake_img, scale_orig_img)\n",
    "      fake_loss = critic(fake_img).mean()\n",
    "      real_loss = -critic(scale_orig_img).mean()\n",
    "      crit_loss =  fake_loss + real_loss + GP_WEIGHT * grad_loss\n",
    "      \n",
    "      optimization_step(crit_loss, crit_optimizer, crit_sched, crit_losses)\n",
    "\n",
    "    # zero gradient before beginning because\n",
    "    # generator was used in the crit. training\n",
    "    for _ in range(GEN_STEP_PER_ITER):\n",
    "      gen_optimizer.zero_grad()\n",
    "\n",
    "      fake_img = generator()\n",
    "\n",
    "      # adversarial & reconstruction loss\n",
    "      adv_loss = -critic(fake_img).mean()\n",
    "      rec_img = generator(z_rec)\n",
    "      rec_loss = F.mse_loss(scale_orig_img, rec_img)\n",
    "      gen_loss = adv_loss + REC_ALPHA * rec_loss\n",
    "      \n",
    "      optimization_step(gen_loss, gen_optimizer, gen_sched, gen_losses)\n",
    "\n",
    "    if step % PRINT_EVERY == 0:\n",
    "      # print some details\n",
    "      print('Step: {}'.format(step))\n",
    "      print('Generator adv: {:.3f}, rec: {:.3f}'.format(adv_loss.item(), rec_loss.item()))\n",
    "      print('Critic fake: {:.3f} real: {:.3f} grad: {:.3f}'.format(fake_loss.item(), real_loss.item(), grad_loss.item()))\n",
    "      if step != 0:\n",
    "        elapsed = time() - last_print\n",
    "        print('Steps per second: {:.2f}'.format(PRINT_EVERY / elapsed))\n",
    "        \n",
    "      # example noise sample at highest scale\n",
    "      with torch.no_grad():\n",
    "        fake_example = generator()\n",
    "      plt.imshow(normed_tensor_to_np_image(fake_example))\n",
    "      plt.show()\n",
    "      last_print = time()\n",
    "\n",
    "  # show the reconstruction at the end of training\n",
    "  print('Reconstruction vs Original:')\n",
    "  with torch.no_grad():\n",
    "    final_rec = generator(z_rec)\n",
    "  plt.subplot(121)\n",
    "  plt.imshow(normed_tensor_to_np_image(final_rec))\n",
    "  plt.subplot(122)\n",
    "  plt.imshow(normed_tensor_to_np_image(scale_orig_img))\n",
    "  plt.show()\n",
    "\n",
    "# save the model when done\n",
    "save_model(SAVE_PATH, original_image, generators, critics, UPSAMPLING_FACTOR, UPSAMPLING_MODE, DOWNSAMPLING_MODE)\n",
    "\n",
    "# show the total time the training took\n",
    "training_duration = time() - training_start\n",
    "print('Total training time in hours: {:.2f}'.format(training_duration / 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ll_rgWhfHFTH"
   },
   "source": [
    "# Load a Pre-trained Model and Sample Qualitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1djfobbolqeZVurdBJv9YQt23wScWlFth"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10708,
     "status": "ok",
     "timestamp": 1590254640409,
     "user": {
      "displayName": "Deniz MERCADIER SAYIN",
      "photoUrl": "",
      "userId": "12243424802177233752"
     },
     "user_tz": -180
    },
    "id": "cmWgUiywx37X",
    "outputId": "d4fbbe09-5412-4878-b550-7143658494f4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training hyper-parameters have no effect here,\n",
    "# all the necessary information is in the model file\n",
    "MODEL_PATH = 'models/birds9.pt'  # model file\n",
    "NUM_SAMPLES = 10  # number of samples to display\n",
    "INPUT_SCALE = 1  # input scale\n",
    "OUTPUT_SIZE = None  # None keeps the size the model was trained with. Only works with input scale 0, try larger values such as (200, 1000)!\n",
    "SEED = 797 # ideally, different from the one used in training\n",
    "OUTPUT_FOLDER = 'samples' # if not None, the folder is created and samples are saved\n",
    "\n",
    "# about the INPUT_SCALE:\n",
    "# 0 is coarsest, 1 one upper etc.\n",
    "# when the input_scale > 0, the\n",
    "# scaled original image is provided as\n",
    "# input to the input_scale and lower\n",
    "# scales are ignored entirely\n",
    "\n",
    "# re-seed for reproducibility\n",
    "seed_rngs(SEED)\n",
    "\n",
    "# load the pre-trained model and maybe set-up custom input\n",
    "gen, original = load_generator(MODEL_PATH, INPUT_SCALE, OUTPUT_SIZE, device=DEVICE)\n",
    "\n",
    "# show the original\n",
    "print('Original: ')\n",
    "plt.imshow(normed_tensor_to_np_image(original))\n",
    "plt.show()\n",
    "\n",
    "# generate and show (and maybe save) samples\n",
    "if OUTPUT_FOLDER:\n",
    "  os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "for i in range(NUM_SAMPLES):\n",
    "  random_sample = gen()\n",
    "  print('Sample {}: '.format(i))\n",
    "  sample_uint = normed_tensor_to_np_image(random_sample)\n",
    "  plt.imshow(sample_uint)\n",
    "  plt.show()\n",
    "  if OUTPUT_FOLDER:\n",
    "    path = os.path.join(OUTPUT_FOLDER, '{}.png'.format(i))\n",
    "    Image.fromarray(sample_uint).save(path, 'PNG')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "an5gjkkzybJp"
   },
   "source": [
    "# Quantitative Results\n",
    "\n",
    "In this section, we aim to reproduce the results of Table 2 (page 7) in the original paper with our own implementation of SIFID (Single-Image Fréchet Inception Distance).\n",
    "\n",
    "However, we cannot hope to reproduce the results exactly. This is because the authors calculated the SIFID values on a survey dataset they prepared for use on Amazon Mechanical Turk. There are 50 different images, with a single high variance and mid variance sample selected for each (probably the best sample they obtained from an unknown number of generated samples). Since we did not have the time to fully train fifty different models and select the best possible sample, we instead opted to do two different things, explained above each cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xEMZkjGIvEWu"
   },
   "source": [
    "### Important note:\n",
    "- Due to an issue in scipy version 1.4+, loading the pre-trained inception model takes too long, and scipy has to be downgraded to 1.3 to ensure proper operation (see the related issue: https://discuss.pytorch.org/t/torchvisions-inception-v3-takes-much-longer-to-load-than-other-models/68756/12). If you are using your own computer, please configure the virtual environment accordingly. If you are using Google colab, you can use the following code cell to downgrade scipy (remember to restart your kernel/runtime afterwards!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3689,
     "status": "ok",
     "timestamp": 1590250939469,
     "user": {
      "displayName": "Deniz MERCADIER SAYIN",
      "photoUrl": "",
      "userId": "12243424802177233752"
     },
     "user_tz": -180
    },
    "id": "xcUi0htNM61c",
    "outputId": "d51d9423-e9a1-4e1e-f466-5a1df09b0e44"
   },
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS CELL UNLESS NECESSARY, AS EXPLAINED ABOVE!!\n",
    "!pip uninstall scipy -y\n",
    "!pip install scipy==1.3.3\n",
    "!pip list | grep scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sifid import *\n",
    "\n",
    "# the first time you run this cell, the pretrained inception model\n",
    "# weights have to be downloaded (around 100 MB), so please be patient!\n",
    "\n",
    "calc = SIFIDCalculator(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dxSaa-N10IKl"
   },
   "source": [
    "### Table 2 Reproduction\n",
    "\n",
    "In this cell, we use the survey images provided by the authors of the paper on github. We calculate the pairwise SIFID values using our own implementation and ensure that it works, as we obtain the same results as shown in the paper on Table 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6278,
     "status": "ok",
     "timestamp": 1590258657784,
     "user": {
      "displayName": "Deniz MERCADIER SAYIN",
      "photoUrl": "",
      "userId": "12243424802177233752"
     },
     "user_tz": -180
    },
    "id": "AsQB3b4SUUTB",
    "outputId": "05a2f73f-3705-4974-a2f8-930c5fb15e48"
   },
   "outputs": [],
   "source": [
    "USER_STUDY_BASE = 'user study'\n",
    "USER_STUDY_REAL = os.path.join(USER_STUDY_BASE, 'real')\n",
    "USER_STUDY_MVAR = os.path.join(USER_STUDY_BASE, 'fake_mid_variance')\n",
    "USER_STUDY_HVAR = os.path.join(USER_STUDY_BASE, 'fake_high_variance')\n",
    "\n",
    "mid_sifid = calc.calculate_average_sifid_folders(USER_STUDY_REAL, USER_STUDY_MVAR)\n",
    "high_sifid = calc.calculate_average_sifid_folders(USER_STUDY_REAL, USER_STUDY_HVAR)\n",
    "\n",
    "print('SIFID at scale N (coarsest): {:.2f}'.format(round(high_sifid, 2)))\n",
    "print('SIFID at scale N-1: {:.2f}'.format(round(mid_sifid, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2n5XxS00eUp"
   },
   "source": [
    "### SIFID on our own models\n",
    "In this section, we manually calculate an average SIFID for each of the models we trained at different input scales over a number of samples. Then, we display each average SIFID on a table along with the average over all our models. Obviously the setting is a little different from the paper due to the small number of models we have, as well as some images having multiple models with different settings, and is also an average over multiple images per model rather than the best sample for each. But we believe that the results are similar enough for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49050,
     "status": "ok",
     "timestamp": 1590252241130,
     "user": {
      "displayName": "Deniz MERCADIER SAYIN",
      "photoUrl": "",
      "userId": "12243424802177233752"
     },
     "user_tz": -180
    },
    "id": "fyWjB-DaHFTX",
    "outputId": "66a653e5-a411-4532-a2d4-717fcca830a2"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "MODEL_DIR = 'models'\n",
    "SAMPLES_PER_MODEL = 50  # around 4 sec per model with a GPU, 5-10 times slower using CPU\n",
    "SEED = 798  # yet another seed for reproducibility of the cell!\n",
    "\n",
    "seed_rngs(SEED)\n",
    "\n",
    "# calculate an average SIFID for each model and scale\n",
    "sifids = []\n",
    "scales = [0, 1]\n",
    "entries = list(os.scandir(MODEL_DIR))\n",
    "for model_entry in tqdm(entries):\n",
    "  row = []\n",
    "  for input_scale in scales:\n",
    "    gen, original = load_generator(model_entry.path, input_scale, device=DEVICE)\n",
    "    avg_sifid = calc.calculate_average_sifid(gen, original, SAMPLES_PER_MODEL)\n",
    "    row.append(avg_sifid)\n",
    "  sifids.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1012,
     "status": "ok",
     "timestamp": 1590256537322,
     "user": {
      "displayName": "Deniz MERCADIER SAYIN",
      "photoUrl": "",
      "userId": "12243424802177233752"
     },
     "user_tz": -180
    },
    "id": "MyM-wHJ8MFTn",
    "outputId": "0b257cd0-a7db-4003-a17a-48a908c5503c"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# calculate the average and round the values for printing\n",
    "models = [entry.name for entry in entries]\n",
    "average_per_scale = np.mean(sifids, axis=0, keepdims=True)\n",
    "table_array = np.concatenate((sifids, average_per_scale), axis=0)\n",
    "table_array = np.around(table_array, 2)\n",
    "\n",
    "# format into table content and print\n",
    "row_header = ['Input Scale', *models, 'Average']\n",
    "col_header = [str(s) for s in scales]\n",
    "table_content = [col_header] + [['{:.2f}'.format(x) for x in row] for row in table_array]\n",
    "\n",
    "fig = go.Figure(data=[go.Table(header=dict(values=row_header),\n",
    "                cells=dict(values=table_content),\n",
    "                columnwidth=40)])\n",
    "fig.update_layout(width=1500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DdtSBD2r2FTQ"
   },
   "source": [
    "# Implementation challenges\n",
    "\n",
    "## Related to the paper\n",
    "\n",
    "Thanks to the detailed treatment of the training process given by the authors both in the original paper and the paper's supplementary material (which can be accessed from the official webpage: https://webee.technion.ac.il/people/tomermic/SinGAN/SinGAN.htm), we knew most of the details before getting into the implementation and did not have too much difficulty, except for a few details: \n",
    "\n",
    "- The most difficult part for us was getting the gradient penalty right. The formulation in the original paper is pretty straightforward, the critic (discriminator) is viewed as a single function, and then the norm of its gradients is used for regularization. Note that the gradients have the shape of the network's input. Let's go over three increasingly difficult cases:\n",
    "  - This is straightforward in the fully connected case, having an $(N, V)$ input with $N$ samples and $V$ dimensions, we can simply compute the norm over each sample and take their average. \n",
    "  - In the convolutional network case this gets a little more confusing, the input is $(N, C, H, W)$ with $C$ being the number of channels and $H$ and $W$ the dimensions (the output is still a scalar fakeness score). In this case, if we take the norm over the whole input (flatten all dimensions except the batch), the gradient norm scales with the size of the image. This can be offset with the multiplier of the gradient penalty in a setting where the input size never changes, but in our case we re-use the weights of the networks for different input images, which is why we need the output to not change too much with the dimensions. Thus, one correct approach is taking the norm only over the channel dimension and then averaging over every pixel of every sample.\n",
    "  - Our case is not the simple discriminator case because we use a patch discriminator which has an image output. A simple approach might be taking the mean of this output to obtain a single scalar, and then apply the standard discriminator approach. However, this fails our purpose, because we take the mean both before getting the gradient and afterwards, which means that our gradient norm will become smaller with larger input sizes rather than not change. Instead, we need to take the sum of the discriminator's output rather than the mean, so that the mean is applied only once. This was the part that took us the longest to figure out before getting the gradient penalty right.\n",
    "\n",
    "We did face a few more issues during the implementation which caused us to chase bugs for a while, but those were not related to the paper or lack of information; they were simply our own small coding mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RAP4ffk82ECB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "run.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
