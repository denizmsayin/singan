{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RPuukm-880dd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess as sp\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhurAwa4KyVJ"
   },
   "source": [
    "## Notes\n",
    "- Every generator and critic at every scale has the same architecture, with 5 Conv(3x3)-BN-LReLU layers.\n",
    "- The patch size is given as 11x11, which comes from the receptive field of the 5 layer deep network.\n",
    "- How the number of kernels change is not exactly clear, paper says \"start with 32, double once every 4 scales\"\n",
    "- It is not clear whether the conv layers use padding (zero, reflect?) to preserve size or not. We assume for now that the generators do preserve the size, but it does not seem necessary for the critics.\n",
    "- How the downsampling is done is not clear from the paper. We assume bicubic interpolation.\n",
    "- For the coarsest scale, authors say that \"the effective receptive field at this level is typically∼1/2of the image’s height\". ~~We assume that this means the input size at the coarsest scale is somewhere between 20 and 25 pixels.~~ The authors later state that they use 25 px for the coarsest and 250 pixels for the finest scale along with a rescaling ratio of 4/3. \n",
    "- Lots of training details are given in the supplementary material.\n",
    "- It is not clear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQbM5wSAgu8z"
   },
   "outputs": [],
   "source": [
    "# arguments cell\n",
    "# image to train on\n",
    "IMG_PATH = 'images/mountains.jpg'\n",
    "\n",
    "# training hyperparameters, \n",
    "# as given in the paper\n",
    "GEN_LEARNING_RATE = 0.0005\n",
    "CRIT_LEARNING_RATE = 0.0005\n",
    "BETA_1 = 0.5\n",
    "BETA_2 = 0.999\n",
    "NUM_ITERS = 2000\n",
    "LR_DROP_STEP = 1600\n",
    "LR_DROP_MULT = 0.1\n",
    "GEN_STEP_PER_ITER = 3\n",
    "CRIT_STEP_PER_ITER = 3\n",
    "REC_ALPHA = 10.0\n",
    "GP_WEIGHT = 0.1\n",
    "\n",
    "# architecture details\n",
    "NUM_SCALES = 9\n",
    "SCALING_FACTOR = 4/3\n",
    "KERNEL_SIZE = 3\n",
    "NUM_BLOCKS = 5\n",
    "INITIAL_KERNEL_COUNT = 32\n",
    "INCREASE_KERNEL_COUNT_EVERY = 4 # SCALES\n",
    "NOISE_BASE_STD = 0.1\n",
    "FIRST_SCALE_NOISE_STD = 1.0\n",
    "MAX_INPUT_SIZE = 250\n",
    "DOWNSAMPLING_MODE = 'bicubic'  # mode used when downscaling the original input\n",
    "UPSAMPLING_MODE = 'bilinear'  # mode used when upscaling during training\n",
    "\n",
    "# extra settings\n",
    "PRINT_EVERY = 50\n",
    "DEVICE = 'cuda'\n",
    "SEED = 796\n",
    "SAVE_DIR = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training cell\n",
    "# closures for easy use depending on settings, so that\n",
    "# we can skip providing every single argument at each iteration\n",
    "def make_generator(kernel_count, noise_std):\n",
    "  sgnet = SGNet(NUM_BLOCKS, kernel_count, KERNEL_SIZE, final_activation=nn.Tanh(), output_channels=3).to(DEVICE)\n",
    "  return SGGen(sgnet, noise_std)\n",
    "  \n",
    "def make_critic(kernel_count):\n",
    "  return SGNet(NUM_BLOCKS, kernel_count, KERNEL_SIZE, final_activation=None, output_channels=1).to(DEVICE)\n",
    "\n",
    "def make_optimizer_and_scheduler(net, net_learning_rate):\n",
    "  optimizer = torch.optim.Adam(net.parameters(), net_learning_rate, (BETA_1, BETA_2))\n",
    "  sched = torch.optim.lr_scheduler.StepLR(optimizer, LR_DROP_STEP, LR_DROP_MULT)\n",
    "  return optimizer, sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "colab_type": "code",
    "id": "5UpI7xyUYJoo",
    "outputId": "d6c53378-63fd-4ccb-f649-d478cc988e7b"
   },
   "outputs": [],
   "source": [
    "# training cell\n",
    "# load the image along with its downsampled versions and their exact sizes\n",
    "scaled_origs, exact_sizes = load_with_reverse_pyramid(IMG_PATH, MAX_INPUT_SIZE, 1/SCALING_FACTOR, NUM_SCALES, \n",
    "                                                      mode=DOWNSAMPLING_MODE, device=DEVICE, verbose=True)\n",
    "  \n",
    "original_image = scaled_origs[-1]\n",
    "print('Input image:')\n",
    "plt.imshow(normed_tensor_to_np_image(original_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "MtiPtGz8x5lN",
    "outputId": "566f8661-8ac6-4bc2-ba59-5365a1227a3d"
   },
   "outputs": [],
   "source": [
    "# training cell\n",
    "# seed stuff\n",
    "seed_rngs(SEED)\n",
    "\n",
    "# create the scaled images\n",
    "coarsest_exact_size = exact_sizes[0]\n",
    "\n",
    "# initialize the constant noise used in reconstruction\n",
    "z_rec_coarsest = FIRST_SCALE_NOISE_STD * torch.randn_like(scaled_origs[0], device=DEVICE)\n",
    "z_rec = [z_rec_coarsest] # a zero tensor is appended after each scale\n",
    "\n",
    "# constant zero input for the coarsest scale during training\n",
    "coarsest_zero_input = torch.zeros_like(z_rec_coarsest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6cDWoeR6fQtS",
    "outputId": "f3f6e382-42e2-45d0-b814-c6ddf55bcde9"
   },
   "outputs": [],
   "source": [
    "# training cell\n",
    "# loop values\n",
    "kernel_count = INITIAL_KERNEL_COUNT\n",
    "generators, critics = [], []\n",
    "gen_losses, crit_losses = [], []\n",
    "for scale_index in range(NUM_SCALES):\n",
    "  print('****************************\\nScale {}'.format(scale_index))\n",
    "\n",
    "  # get the original image at the current scale\n",
    "  scale_orig_img = scaled_origs[scale_index]\n",
    "\n",
    "  # things to be done after the first scale\n",
    "  if scale_index > 0:\n",
    "    # use RMSE to determine the standard deviation of the input noise\n",
    "    with torch.no_grad():\n",
    "      reconstruction = generator(z_rec)  # specific reconstruction noise\n",
    "    scaled_reconstruction, _ = exact_interpolate(reconstruction, SCALING_FACTOR, exact_sizes[scale_index-1], UPSAMPLING_MODE)\n",
    "    rmse = torch.sqrt(F.mse_loss(scaled_reconstruction, scale_orig_img))\n",
    "    print('RMSE: {:.2f}'.format(rmse))\n",
    "    # if the scale matches, increase kernel count\n",
    "    if scale_index % INCREASE_KERNEL_COUNT_EVERY == 0:\n",
    "      kernel_count *= 2\n",
    "    # add a zero tensor to the reconstruction noise list\n",
    "    # since it is defined as [z*, 0, 0, 0...] for some z*\n",
    "    z_rec.append(torch.zeros_like(scale_orig_img))\n",
    "      \n",
    "  # create the noise sampler based on the RMSE\n",
    "  # the first scale's stdev is different due to the zero input,\n",
    "  # the noise has to be much stronger than in the upper layers\n",
    "  scale_noise_std = FIRST_SCALE_NOISE_STD if scale_index == 0 else rmse * NOISE_BASE_STD\n",
    "\n",
    "  ## initialize the generator\n",
    "  # create a generator for this specific scale and initialize it\n",
    "  scale_generator = make_generator(kernel_count, scale_noise_std)\n",
    "  # copy weights from previous if possible, and add to the list\n",
    "  initialize_net(scale_generator, generators)\n",
    "  \n",
    "  # create a single generator view from the stack of generators\n",
    "  generic_generator = MultiScaleSGGenView(generators, SCALING_FACTOR, UPSAMPLING_MODE)\n",
    "  # fix the input parameters for easier forward calls\n",
    "  generator = FixedInputSGGenView(generic_generator, coarsest_zero_input, coarsest_exact_size)\n",
    "  \n",
    "  ## initialize the critic (discriminator)\n",
    "  critic = make_critic(kernel_count)\n",
    "  initialize_net(critic, critics)\n",
    "\n",
    "  # create the optimizers and schedulers\n",
    "  gen_optimizer, gen_sched = make_optimizer_and_scheduler(generator, GEN_LEARNING_RATE)\n",
    "  crit_optimizer, crit_sched = make_optimizer_and_scheduler(critic, CRIT_LEARNING_RATE)\n",
    "\n",
    "  # print norms to ensure correct operation\n",
    "  gen_norms = ['G{}: {:.3f}'.format(i, sum_param_norms(g)) for i, g in enumerate(generators)]\n",
    "  crit_norms = ['C{}: {:.3f}'.format(i, sum_param_norms(c)) for i, c in enumerate(critics)]\n",
    "  print('Generator norms: ' + ', '.join(gen_norms))\n",
    "  print('Critic norms: ' + ', '.join(crit_norms))\n",
    "  \n",
    "  # perform training\n",
    "  for step in range(NUM_ITERS):\n",
    "\n",
    "    for _ in range(CRIT_STEP_PER_ITER):\n",
    "      crit_optimizer.zero_grad()\n",
    "      \n",
    "      # the model handles noise sampling on its own\n",
    "      fake_img = generator()\n",
    "      \n",
    "      # gradient & adversarial loss\n",
    "      grad_loss = gradient_penalty(critic, fake_img, scale_orig_img)\n",
    "      fake_loss = critic(fake_img).mean()\n",
    "      real_loss = -critic(scale_orig_img).mean()\n",
    "      crit_loss =  fake_loss + real_loss + GP_WEIGHT * grad_loss\n",
    "      \n",
    "      optimization_step(crit_loss, crit_optimizer, crit_sched, crit_losses)\n",
    "\n",
    "    # zero gradient before beginning because\n",
    "    # generator was used in the crit. training\n",
    "    for _ in range(GEN_STEP_PER_ITER):\n",
    "      gen_optimizer.zero_grad()\n",
    "\n",
    "      fake_img = generator()\n",
    "\n",
    "      # adversarial & reconstruction loss\n",
    "      adv_loss = -critic(fake_img).mean()\n",
    "      rec_img = generator(z_rec)\n",
    "      rec_loss = F.mse_loss(scale_orig_img, rec_img)\n",
    "      gen_loss = adv_loss + REC_ALPHA * rec_loss\n",
    "      \n",
    "      optimization_step(gen_loss, gen_optimizer, gen_sched, gen_losses)\n",
    "\n",
    "    if step % PRINT_EVERY == 0:\n",
    "      # print some details\n",
    "      print('Step: {}'.format(step))\n",
    "      print('Generator adv: {:.3f}, rec: {:.3f}'.format(adv_loss.item(), rec_loss.item()))\n",
    "      print('Critic fake: {:.3f} real: {:.3f} grad: {:.3f}'.format(fake_loss.item(), real_loss.item(), grad_loss.item()))\n",
    "      if step != 0:\n",
    "        elapsed = time() - last_print\n",
    "        print('Steps per second: {:.2f}'.format(PRINT_EVERY / elapsed))\n",
    "        \n",
    "      # example noise sample at highest scale\n",
    "      with torch.no_grad():\n",
    "        fake_example = generator()\n",
    "      plt.imshow(normed_tensor_to_np_image(fake_example))\n",
    "      plt.show()\n",
    "      last_print = time()\n",
    "\n",
    "  # show the reconstruction at the end of training\n",
    "  print('Reconstruction vs Original:')\n",
    "  with torch.no_grad():\n",
    "    final_rec = generator(z_rec)\n",
    "  plt.subplot(121)\n",
    "  plt.imshow(normed_tensor_to_np_image(final_rec))\n",
    "  plt.subplot(122)\n",
    "  plt.imshow(normed_tensor_to_np_image(scale_orig_img))\n",
    "  plt.show()\n",
    "\n",
    "# save the model when done\n",
    "save_path = get_model_path(SAVE_DIR, IMG_PATH)\n",
    "save_model(save_path, generators, critics, SCALING_FACTOR, UPSAMPLING_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cmWgUiywx37X"
   },
   "outputs": [],
   "source": [
    "# can modify IMG_PATH here if necessary\n",
    "# IMG_PATH = 'images/birds.png'\n",
    "\n",
    "IMG_PATH = 'images/birds9.png'\n",
    "NUM_SAMPLES = 100\n",
    "INPUT_SCALE = 0\n",
    "SEED = 797 # different from the first one\n",
    "\n",
    "# about the INPUT_SCALE:\n",
    "# 0 is coarsest, 1 one upper etc.\n",
    "# when the input_scale > 0, the\n",
    "# scaled original image is provided as\n",
    "# input to the input_scale and lower\n",
    "# scales are ignored entirely\n",
    "\n",
    "# re-seed for reproducibility\n",
    "seed_rngs(SEED)\n",
    "\n",
    "# load the saved model, critics are not necessary\n",
    "model_path = get_model_path(SAVE_DIR, IMG_PATH)\n",
    "generators, _, scaling_factor, scaling_mode = load_model(model_path)\n",
    "generic_gen = MultiScaleSGGenView(generators[INPUT_SCALE:], scaling_factor, scaling_mode)\n",
    "\n",
    "# load the image pyramid\n",
    "scaled_origs, exact_sizes = load_with_reverse_pyramid(IMG_PATH, MAX_INPUT_SIZE, 1/scaling_factor, len(generators), \n",
    "                                                      mode=DOWNSAMPLING_MODE, device=DEVICE, verbose=True)\n",
    "\n",
    "\n",
    "# fix the input of the model, by default the downsampled original image\n",
    "gen_input = scaled_origs[INPUT_SCALE]\n",
    "gen_size = exact_sizes[INPUT_SCALE]\n",
    "if INPUT_SCALE == 0: # nothing at the lowest scale\n",
    "  gen_input = torch.zeros_like(gen_input)\n",
    "gen = FixedInputSGGenView(generic_gen, gen_input, gen_size)\n",
    "\n",
    "# generate samples\n",
    "print('Original: ')\n",
    "plt.imshow(normed_tensor_to_np_image(scaled_origs[-1]))\n",
    "plt.show()\n",
    "for i in range(NUM_SAMPLES):\n",
    "  with torch.no_grad():\n",
    "    random_sample = gen()\n",
    "  print('Sample {}: '.format(i))\n",
    "  plt.imshow(normed_tensor_to_np_image(random_sample))\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "run.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
