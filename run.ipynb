{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RPuukm-880dd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess as sp\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhurAwa4KyVJ"
   },
   "source": [
    "## Notes\n",
    "- Every generator and critic at every scale has the same architecture, with 5 Conv(3x3)-BN-LReLU layers.\n",
    "- The patch size is given as 11x11, which comes from the receptive field of the 5 layer deep network.\n",
    "- How the number of kernels change is not exactly clear, paper says \"start with 32, double once every 4 scales\"\n",
    "- It is not clear whether the conv layers use padding (zero, reflect?) to preserve size or not. We assume for now that the generators do preserve the size, but it does not seem necessary for the critics.\n",
    "- How the downsampling is done is not clear from the paper. We assume bicubic interpolation.\n",
    "- For the coarsest scale, authors say that \"the effective receptive field at this level is typically∼1/2of the image’s height\". ~~We assume that this means the input size at the coarsest scale is somewhere between 20 and 25 pixels.~~ The authors later state that they use 25 px for the coarsest and 250 pixels for the finest scale along with a rescaling ratio of 4/3. \n",
    "- Lots of training details are given in the supplementary material.\n",
    "- It is not clear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Y9niqk59UT-"
   },
   "outputs": [],
   "source": [
    "def prepare_bsds300():\n",
    "  if os.path.isdir('BSDS300'):\n",
    "    print('Dataset already downloaded')\n",
    "  else:\n",
    "    sp.call(('wget', 'https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz'))\n",
    "    sp.call(('tar', '-xzf', 'BSDS300-images.tgz'))\n",
    "    sp.call(('rm', '-f', 'BSDS300-images.tgz'))\n",
    "    print('Downloaded dataset successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQbM5wSAgu8z"
   },
   "outputs": [],
   "source": [
    "# as given in the paper\n",
    "LEARNING_RATE = 0.0005\n",
    "BETA_1 = 0.5\n",
    "BETA_2 = 0.999\n",
    "NUM_ITERS = 100\n",
    "LR_DROP_STEP = 1600\n",
    "LR_DROP_MULT = 0.1\n",
    "GEN_STEP_PER_ITER = 3\n",
    "DIS_STEP_PER_ITER = 3\n",
    "REC_ALPHA = 10.0\n",
    "GP_WEIGHT = 0.1 # paper: 0.1\n",
    "NUM_SCALES = 9\n",
    "SCALING_FACTOR = 4/3\n",
    "INITIAL_KERNEL_COUNT = 32\n",
    "INCREASE_KERNEL_COUNT_EVERY = 4 # SCALES\n",
    "NOISE_BASE_STD = 0.1\n",
    "FIRST_SCALE_NOISE_STD = 1.0\n",
    "START_SIZE = (250, 250)\n",
    "SCALING_MODE = 'bilinear'\n",
    "\n",
    "\n",
    "PRINT_EVERY = 50\n",
    "DEVICE = 'cuda'\n",
    "SEED = 796\n",
    "\n",
    "conv2d_initializer = None\n",
    "batch_norm_initializer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "colab_type": "code",
    "id": "5UpI7xyUYJoo",
    "outputId": "d6c53378-63fd-4ccb-f649-d478cc988e7b"
   },
   "outputs": [],
   "source": [
    "prepare_bsds300()\n",
    "\n",
    "DS_DIR = 'BSDS300/images/train'\n",
    "EXAMPLE_IDX = 11\n",
    "IMG_LIST = sorted(os.listdir(DS_DIR))\n",
    "EXAMPLE_IMG_PATH = os.path.join(DS_DIR, IMG_LIST[EXAMPLE_IDX])\n",
    "\n",
    "orig_img_uint = np.array(Image.open(EXAMPLE_IMG_PATH).resize(START_SIZE, Image.BICUBIC))\n",
    "plt.imshow(orig_img_uint)\n",
    "print(EXAMPLE_IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "MtiPtGz8x5lN",
    "outputId": "566f8661-8ac6-4bc2-ba59-5365a1227a3d"
   },
   "outputs": [],
   "source": [
    "# seed stuff\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# create the scaled images\n",
    "original_image = np_image_to_normed_tensor(orig_img_uint).to(DEVICE)\n",
    "scaled_original_images, exact_scale_sizes = create_scale_pyramid(original_image, 1/SCALING_FACTOR, NUM_SCALES, SCALING_MODE)\n",
    "scaled_original_images, exact_scale_sizes = scaled_original_images[::-1], exact_scale_sizes[::-1]  # reverse since we start from coarsest\n",
    "coarsest_exact_size = exact_scale_sizes[0]\n",
    "print(exact_scale_sizes)\n",
    "print([q.shape for q in scaled_original_images])\n",
    "\n",
    "# initialize the constant noise used in reconstruction\n",
    "z_rec_coarsest = FIRST_SCALE_NOISE_STD * torch.randn_like(scaled_original_images[0], device=DEVICE)\n",
    "z_rec = [z_rec_coarsest] # a zero tensor is appended after each scale\n",
    "\n",
    "# constant zero input for the coarsest scale during training\n",
    "coarsest_zero_input = torch.zeros_like(z_rec_coarsest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6cDWoeR6fQtS",
    "outputId": "f3f6e382-42e2-45d0-b814-c6ddf55bcde9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# loop values\n",
    "kernel_count = INITIAL_KERNEL_COUNT\n",
    "generators, critics, noise_samplers = [], [], []\n",
    "gen_losses, crit_losses = [], []\n",
    "rmse = 1 # initial noise multiplier\n",
    "for scale_index in range(NUM_SCALES):\n",
    "  print('****************************\\nScale {}'.format(scale_index))\n",
    "\n",
    "\n",
    "  # get the original image at the necessary scale for reconstruction\n",
    "  scale_orig_img = scaled_original_images[scale_index]\n",
    "\n",
    "  # if a previous generator exists, use RMSE to determine\n",
    "  # the standard deviation of the input noise\n",
    "  if scale_index > 0:\n",
    "    reconstruction = generator(coarsest_zero_input, coarsest_exact_size, z_rec)\n",
    "    scaled_reconstruction, _ = exact_interpolate(reconstruction, exact_scale_sizes[scale_index-1], SCALING_FACTOR, SCALING_MODE)\n",
    "    rmse = torch.sqrt(F.mse_loss(scaled_reconstruction, scale_orig_img)).item()\n",
    "\n",
    "  # create the noise sampler based on the RMSE\n",
    "  # BUG: same noise_sampler is used for every scale, oof!\n",
    "  scale_noise_std = FIRST_SCALE_NOISE_STD if scale_index == 0 else rmse * NOISE_BASE_STD\n",
    "  scale_noise_sampler = lambda x: scale_noise_std * torch.randn_like(x, device=DEVICE)\n",
    "  noise_samplers.append(scale_noise_sampler)\n",
    "  print('RMSE: {:.2f}'.format(rmse))\n",
    "\n",
    "  # renew architectures if it is time to change kernel sizes\n",
    "  architecture_changed = False\n",
    "  if scale_index != 0 and scale_index % INCREASE_KERNEL_COUNT_EVERY == 0:\n",
    "    kernel_count *= 2\n",
    "    architecture_changed = True\n",
    "\n",
    "  ## initialize the generator\n",
    "  # create a generator for this specific scale and initialize it\n",
    "  scale_generator = SGNet(output_channels=3, kernel_count=kernel_count, final_activation=nn.Tanh(), \n",
    "                          conv_init=conv2d_initializer, bn_init=batch_norm_initializer).to(DEVICE)\n",
    "  init_net(scale_generator, generators, architecture_changed)\n",
    "  # create a single generator view from the stack of generators\n",
    "  generator = MultiScaleSGNetView(generators, SCALING_FACTOR, noise_samplers)\n",
    "\n",
    "  ## initialize the critic (discriminator)\n",
    "  # the authors recommend initializing the discriminator using the weights of\n",
    "  # the previous one unless the number of kernels is changed\n",
    "  critic = SGNet(output_channels=1, kernel_count=kernel_count, final_activation=None, \n",
    "                 conv_init=conv2d_initializer, bn_init=batch_norm_initializer).to(DEVICE) # patch critic\n",
    "  init_net(critic, critics, architecture_changed)\n",
    "  \n",
    "  # weight of gen0 to ensure it is not trained\n",
    "  print('Norm sums:')\n",
    "  for i, gen in enumerate(generators):\n",
    "    wbs = get_weights_and_biases(gen)\n",
    "    norm = torch.tensor([torch.norm(x) for x in wbs]).sum().item()\n",
    "    print('Gen-{} norm: {:.2f}'.format(i, norm))\n",
    "  print('Critic norm: {:.2f}'.format(torch.tensor([torch.norm(x) for x in get_weights_and_biases(critic)]).sum().item()))\n",
    "\n",
    "  # re-create the optimizers and schedulers\n",
    "  gen_optimizer = torch.optim.Adam(generator.parameters(), LEARNING_RATE, (BETA_1, BETA_2))\n",
    "  gen_sched = torch.optim.lr_scheduler.StepLR(gen_optimizer, LR_DROP_STEP, LR_DROP_MULT)\n",
    "\n",
    "  crit_optimizer = torch.optim.Adam(critic.parameters(), LEARNING_RATE, (BETA_1, BETA_2))\n",
    "  crit_sched = torch.optim.lr_scheduler.StepLR(crit_optimizer, LR_DROP_STEP, LR_DROP_MULT)\n",
    "\n",
    "  # add a zero tensor to the reconstruction noise list\n",
    "  # since it is defined as [z*, 0, 0, 0...] for some z*\n",
    "  if scale_index != 0:\n",
    "    z_rec.append(torch.zeros_like(scale_orig_img))\n",
    "\n",
    "  # perform training\n",
    "\n",
    "  print('Start')\n",
    "  fake_img = generator(coarsest_zero_input, coarsest_exact_size)\n",
    "  plt.imshow(normed_tensor_to_np_image(fake_img))\n",
    "  plt.show()\n",
    "\n",
    "  for step in range(NUM_ITERS):\n",
    "\n",
    "    for _ in range(DIS_STEP_PER_ITER):\n",
    "      crit_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "      # adversarial loss\n",
    "      # take a sample from the line between the real and generated images\n",
    "      # for use in the gradient penalty (Impr. Training of WGANs)\n",
    "\n",
    "      # another thing that is not very clear is the WGAN-GP loss in this context.\n",
    "      # In the case of a scalar discr. output, what should be done is simply\n",
    "      # norm the gradient (image-shaped) across the channel axis, and take \n",
    "      # the mean across all pixels.\n",
    "      # In this case, the output of the critic (discr) is an image (PatchGAN).\n",
    "      # If we take its mean to obtain a scalar and then apply the same approach\n",
    "      # as the scalar output discr., it seems to supress the penalty twice \n",
    "      # (as if the mean was applied twice). Instead, taking the sum of the\n",
    "      # output allows us to apply the mean only once, which we believe is the\n",
    "      # proper normalization.\n",
    "      fake_img = generator(coarsest_zero_input, coarsest_exact_size)\n",
    "      '''\n",
    "      epsilon = np.random.uniform(0, 1)\n",
    "      grad_sample = epsilon * scale_orig_img + (1 - epsilon) * fake_img\n",
    "      f_grad_sample = critic(grad_sample).sum()\n",
    "      grad, = torch.autograd.grad(f_grad_sample, grad_sample, create_graph=True, retain_graph=True)\n",
    "      grad_loss = ((torch.norm(grad, 2, dim=1) - 1)**2).mean() # mean over batch\n",
    "      '''\n",
    "      grad_loss = gradient_penalty(critic, fake_img, scale_orig_img)\n",
    "      # calculate the f losses and sum up\n",
    "      fake_loss = critic(fake_img).mean()\n",
    "      real_loss = -critic(scale_orig_img).mean()\n",
    "      crit_loss =  fake_loss + real_loss + GP_WEIGHT * grad_loss # TODO: are signs of fake and real loss correct? \n",
    "      \n",
    "      # optimize\n",
    "      crit_loss.backward()\n",
    "      crit_optimizer.step()\n",
    "      crit_sched.step()\n",
    "      crit_losses.append(crit_loss.item())\n",
    "\n",
    "    for _ in range(GEN_STEP_PER_ITER):\n",
    "      gen_optimizer.zero_grad()\n",
    "\n",
    "      # always provide zero input to the coarsest scale\n",
    "      # the model handles noise sampling on its own\n",
    "      fake_img = generator(coarsest_zero_input, coarsest_exact_size)\n",
    "\n",
    "      # adversarial loss\n",
    "      adv_loss = -critic(fake_img).mean()\n",
    "\n",
    "      # reconstruction loss\n",
    "      rec_img = generator(coarsest_zero_input, coarsest_exact_size, z_rec)\n",
    "      rec_loss = F.mse_loss(scale_orig_img, rec_img)\n",
    "\n",
    "      # total loss & optimization\n",
    "      gen_loss = adv_loss + REC_ALPHA * rec_loss\n",
    "      gen_loss.backward()\n",
    "      gen_optimizer.step()\n",
    "      gen_sched.step()\n",
    "      gen_losses.append(gen_loss.item())\n",
    "\n",
    "    if step % PRINT_EVERY == 0:\n",
    "      print('Step: {}'.format(step))\n",
    "      print('Generator adv: {:.3f}, rec: {:.3f}'.format(adv_loss.item(), rec_loss.item()))\n",
    "      print('Critic fake: {:.3f} real: {:.3f} grad: {:.3f}'.format(fake_loss.item(), real_loss.item(), grad_loss.item()))\n",
    "      if step != 0:\n",
    "        elapsed = time() - last_print\n",
    "        print('Steps per second: {:.2f}'.format(PRINT_EVERY / elapsed))\n",
    "      # example noise sample at highest scale\n",
    "      with torch.no_grad():\n",
    "        fake_example, fake_input = generator(coarsest_zero_input, coarsest_exact_size, return_input=True)\n",
    "      plt.subplot(121)\n",
    "      plt.imshow(normed_tensor_to_np_image(fake_example))\n",
    "      plt.subplot(122)\n",
    "      plt.imshow(normed_tensor_to_np_image(fake_input))\n",
    "      plt.show()\n",
    "      last_print = time()\n",
    "\n",
    "  print('Reconstruction:')\n",
    "  with torch.no_grad():\n",
    "    final_rec = generator(coarsest_zero_input, coarsest_exact_size, z_rec)\n",
    "  plt.imshow(normed_tensor_to_np_image(final_rec))\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cmWgUiywx37X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "Yau90mTBW0oU",
    "outputId": "bb248adc-5b2f-4174-d2f8-1fd7e07f2dcf"
   },
   "outputs": [],
   "source": [
    "plt.plot(crit_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9jvvjje_WoUX"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zenvyZsfjCRm"
   },
   "outputs": [],
   "source": [
    "print(SGNet(output_channels=1, kernel_count=kernel_count, final_activation=nn.Identity()).to(DEVICE)(torch.ones(1, 3, 5, 5).cuda()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "run.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
