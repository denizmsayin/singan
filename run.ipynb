{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RPuukm-880dd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess as sp\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhurAwa4KyVJ"
   },
   "source": [
    "## Notes\n",
    "- Every generator and critic at every scale has the same architecture, with 5 Conv(3x3)-BN-LReLU layers.\n",
    "- The patch size is given as 11x11, which comes from the receptive field of the 5 layer deep network.\n",
    "- How the number of kernels change is not exactly clear, paper says \"start with 32, double once every 4 scales\"\n",
    "- It is not clear whether the conv layers use padding (zero, reflect?) to preserve size or not. We assume for now that the generators do preserve the size, but it does not seem necessary for the critics.\n",
    "- How the downsampling is done is not clear from the paper. We assume bicubic interpolation.\n",
    "- For the coarsest scale, authors say that \"the effective receptive field at this level is typically∼1/2of the image’s height\". ~~We assume that this means the input size at the coarsest scale is somewhere between 20 and 25 pixels.~~ The authors later state that they use 25 px for the coarsest and 250 pixels for the finest scale along with a rescaling ratio of 4/3. \n",
    "- Lots of training details are given in the supplementary material.\n",
    "- It is not clear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQbM5wSAgu8z"
   },
   "outputs": [],
   "source": [
    "# image to train on\n",
    "IMG_PATH = 'images/birds.png'\n",
    "\n",
    "# training hyperparameters, \n",
    "# as given in the paper\n",
    "GEN_LEARNING_RATE = 0.0005\n",
    "CRIT_LEARNING_RATE = 0.0005\n",
    "BETA_1 = 0.5\n",
    "BETA_2 = 0.999\n",
    "NUM_ITERS = 2000\n",
    "LR_DROP_STEP = 1600\n",
    "LR_DROP_MULT = 0.1\n",
    "GEN_STEP_PER_ITER = 3\n",
    "CRIT_STEP_PER_ITER = 3\n",
    "REC_ALPHA = 10.0\n",
    "GP_WEIGHT = 0.1\n",
    "\n",
    "# architecture details\n",
    "NUM_SCALES = 5\n",
    "SCALING_FACTOR = 4/3\n",
    "KERNEL_SIZE = 3\n",
    "NUM_BLOCKS = 5\n",
    "INITIAL_KERNEL_COUNT = 32\n",
    "INCREASE_KERNEL_COUNT_EVERY = 4 # SCALES\n",
    "NOISE_BASE_STD = 0.1\n",
    "FIRST_SCALE_NOISE_STD = 1.0\n",
    "MAX_INPUT_SIZE = 250\n",
    "SCALING_MODE = 'bilinear'\n",
    "\n",
    "# extra settings\n",
    "PRINT_EVERY = 50\n",
    "DEVICE = 'cuda'\n",
    "SEED = 796\n",
    "SAVE_DIR = 'models/fox'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closures for easy use depending on settings, so that\n",
    "# we can skip providing every single argument at each iteration\n",
    "def make_generator(kernel_count, noise_std):\n",
    "  sgnet = SGNet(NUM_BLOCKS, kernel_count, KERNEL_SIZE, final_activation=nn.Tanh(), output_channels=3).to(DEVICE)\n",
    "  return SGGen(sgnet, noise_std)\n",
    "  \n",
    "def make_critic(kernel_count):\n",
    "  return SGNet(NUM_BLOCKS, kernel_count, KERNEL_SIZE, final_activation=None, output_channels=1).to(DEVICE)\n",
    "\n",
    "def make_optimizer_and_scheduler(net, net_learning_rate):\n",
    "  optimizer = torch.optim.Adam(net.parameters(), net_learning_rate, (BETA_1, BETA_2))\n",
    "  sched = torch.optim.lr_scheduler.StepLR(optimizer, LR_DROP_STEP, LR_DROP_MULT)\n",
    "  return optimizer, sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "colab_type": "code",
    "id": "5UpI7xyUYJoo",
    "outputId": "d6c53378-63fd-4ccb-f649-d478cc988e7b"
   },
   "outputs": [],
   "source": [
    "# both np uint and tensor versions\n",
    "orig_img_uint = np.array(Image.open(IMG_PATH).convert('RGB'))\n",
    "orig_img = np_image_to_normed_tensor(orig_img_uint)\n",
    "input_img_uint = orig_img_uint\n",
    "input_img = orig_img\n",
    "  \n",
    "# resize the image to max size if necessary\n",
    "orig_h, orig_w, _ = orig_img_uint.shape\n",
    "if orig_h > MAX_INPUT_SIZE or orig_w > MAX_INPUT_SIZE:\n",
    "  input_img = torch.clamp(resize_long_edge(orig_img, MAX_INPUT_SIZE), -1, 1)\n",
    "  input_img_uint = normed_tensor_to_np_image(input_img)\n",
    "  input_h, input_w, _ = input_img_uint.shape\n",
    "  print('Resized image from {}x{} to {}x{}'.format(orig_h, orig_w, input_h, input_w))\n",
    "  \n",
    "print('Input image:')\n",
    "plt.imshow(input_img_uint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "MtiPtGz8x5lN",
    "outputId": "566f8661-8ac6-4bc2-ba59-5365a1227a3d"
   },
   "outputs": [],
   "source": [
    "# seed stuff\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# create the scaled images\n",
    "original_image = input_img.to(DEVICE)\n",
    "scaled_original_images, exact_scale_sizes = create_scale_pyramid(original_image, 1/SCALING_FACTOR, NUM_SCALES, SCALING_MODE)\n",
    "scaled_original_images, exact_scale_sizes = scaled_original_images[::-1], exact_scale_sizes[::-1]  # reverse since we start from coarsest\n",
    "coarsest_exact_size = exact_scale_sizes[0]\n",
    "\n",
    "# initialize the constant noise used in reconstruction\n",
    "z_rec_coarsest = FIRST_SCALE_NOISE_STD * torch.randn_like(scaled_original_images[0], device=DEVICE)\n",
    "z_rec = [z_rec_coarsest] # a zero tensor is appended after each scale\n",
    "\n",
    "# constant zero input for the coarsest scale during training\n",
    "coarsest_zero_input = torch.zeros_like(z_rec_coarsest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6cDWoeR6fQtS",
    "outputId": "f3f6e382-42e2-45d0-b814-c6ddf55bcde9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# loop values\n",
    "kernel_count = INITIAL_KERNEL_COUNT\n",
    "generators, critics = [], []\n",
    "gen_losses, crit_losses = [], []\n",
    "for scale_index in range(NUM_SCALES):\n",
    "  print('****************************\\nScale {}'.format(scale_index))\n",
    "\n",
    "  # get the original image at the current scale\n",
    "  scale_orig_img = scaled_original_images[scale_index]\n",
    "\n",
    "  # things to be done after the first scale\n",
    "  if scale_index > 0:\n",
    "    # use RMSE to determine the standard deviation of the input noise\n",
    "    with torch.no_grad():\n",
    "      reconstruction = generator(z_rec)  # specific reconstruction noise\n",
    "    scaled_reconstruction, _ = exact_interpolate(reconstruction, SCALING_FACTOR, exact_scale_sizes[scale_index-1], SCALING_MODE)\n",
    "    rmse = torch.sqrt(F.mse_loss(scaled_reconstruction, scale_orig_img))\n",
    "    print('RMSE: {:.2f}'.format(rmse))\n",
    "    # if the scale matches, increase kernel count\n",
    "    if scale_index % INCREASE_KERNEL_COUNT_EVERY == 0:\n",
    "      kernel_count *= 2\n",
    "    # add a zero tensor to the reconstruction noise list\n",
    "    # since it is defined as [z*, 0, 0, 0...] for some z*\n",
    "    z_rec.append(torch.zeros_like(scale_orig_img))\n",
    "      \n",
    "  # create the noise sampler based on the RMSE\n",
    "  # the first scale's stdev is different due to the zero input,\n",
    "  # the noise has to be much stronger than in the upper layers\n",
    "  scale_noise_std = FIRST_SCALE_NOISE_STD if scale_index == 0 else rmse * NOISE_BASE_STD\n",
    "\n",
    "  ## initialize the generator\n",
    "  # create a generator for this specific scale and initialize it\n",
    "  scale_generator = make_generator(kernel_count, scale_noise_std)\n",
    "  # copy weights from previous if possible, and add to the list\n",
    "  initialize_net(scale_generator, generators)\n",
    "  \n",
    "  # create a single generator view from the stack of generators\n",
    "  generic_generator = MultiScaleSGGenView(generators, SCALING_FACTOR, SCALING_MODE)\n",
    "  # the generator works with any input size, however we do not\n",
    "  # want to keep repeating the same arguments since we always\n",
    "  # use the same input size while training one scale -> closure-like\n",
    "  # cannot use a lambda like: \n",
    "  # generator = lambda z=None: generic_generator(coarsest_zero_input, coarsest_exact_size, z)\n",
    "  # here because .parameters() are not registered and generator cannot be used as a pytorch model\n",
    "  generator = FixedSizeSGGenView(generic_generator, coarsest_zero_input, coarsest_exact_size)\n",
    "  \n",
    "  ## initialize the critic (discriminator)\n",
    "  critic = make_critic(kernel_count)\n",
    "  initialize_net(critic, critics)\n",
    "\n",
    "  # create the optimizers and schedulers\n",
    "  gen_optimizer, gen_sched = make_optimizer_and_scheduler(generator, GEN_LEARNING_RATE)\n",
    "  crit_optimizer, crit_sched = make_optimizer_and_scheduler(critic, CRIT_LEARNING_RATE)\n",
    "\n",
    "  # print norms to ensure correct operation\n",
    "  gen_norms = ['G{}: {:.3f}'.format(i, sum_param_norms(g)) for i, g in enumerate(generators)]\n",
    "  crit_norms = ['C{}: {:.3f}'.format(i, sum_param_norms(c)) for i, c in enumerate(critics)]\n",
    "  print('Generator norms: ' + ', '.join(gen_norms))\n",
    "  print('Critic norms: ' + ', '.join(crit_norms))\n",
    "  \n",
    "  # perform training\n",
    "  for step in range(NUM_ITERS):\n",
    "\n",
    "    for _ in range(CRIT_STEP_PER_ITER):\n",
    "      crit_optimizer.zero_grad()\n",
    "      \n",
    "      # the model handles noise sampling on its own\n",
    "      fake_img = generator()\n",
    "      \n",
    "      # gradient & adversarial loss\n",
    "      grad_loss = gradient_penalty(critic, fake_img, scale_orig_img)\n",
    "      fake_loss = critic(fake_img).mean()\n",
    "      real_loss = -critic(scale_orig_img).mean()\n",
    "      crit_loss =  fake_loss + real_loss + GP_WEIGHT * grad_loss\n",
    "      \n",
    "      optimization_step(crit_loss, crit_optimizer, crit_sched, crit_losses)\n",
    "\n",
    "    # zero gradient before beginning because\n",
    "    # generator was used in the crit. training\n",
    "    for _ in range(GEN_STEP_PER_ITER):\n",
    "      gen_optimizer.zero_grad()\n",
    "\n",
    "      fake_img = generator()\n",
    "\n",
    "      # adversarial & reconstruction loss\n",
    "      adv_loss = -critic(fake_img).mean()\n",
    "      rec_img = generator(z_rec)\n",
    "      rec_loss = F.mse_loss(scale_orig_img, rec_img)\n",
    "      gen_loss = adv_loss + REC_ALPHA * rec_loss\n",
    "      \n",
    "      optimization_step(gen_loss, gen_optimizer, gen_sched, gen_losses)\n",
    "\n",
    "    if step % PRINT_EVERY == 0:\n",
    "      # print some details\n",
    "      print('Step: {}'.format(step))\n",
    "      print('Generator adv: {:.3f}, rec: {:.3f}'.format(adv_loss.item(), rec_loss.item()))\n",
    "      print('Critic fake: {:.3f} real: {:.3f} grad: {:.3f}'.format(fake_loss.item(), real_loss.item(), grad_loss.item()))\n",
    "      if step != 0:\n",
    "        elapsed = time() - last_print\n",
    "        print('Steps per second: {:.2f}'.format(PRINT_EVERY / elapsed))\n",
    "        \n",
    "      # example noise sample at highest scale\n",
    "      with torch.no_grad():\n",
    "        fake_example = generator()\n",
    "      plt.imshow(normed_tensor_to_np_image(fake_example))\n",
    "      plt.show()\n",
    "      last_print = time()\n",
    "\n",
    "  # show the reconstruction at the end of training\n",
    "  print('Reconstruction:')\n",
    "  with torch.no_grad():\n",
    "    final_rec = generator(z_rec)\n",
    "  plt.imshow(normed_tensor_to_np_image(final_rec))\n",
    "  plt.show()\n",
    "\n",
    "# save the model when done\n",
    "save_model(SAVE_DIR, generators, critics, SCALING_FACTOR, SCALING_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cmWgUiywx37X"
   },
   "outputs": [],
   "source": [
    "generators, _, scaling_factor, scaling_mode = load_model(SAVE_DIR)\n",
    "generators"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "run.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
